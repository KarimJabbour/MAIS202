{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-05T11:21:53.698035Z","iopub.execute_input":"2022-04-05T11:21:53.69836Z","iopub.status.idle":"2022-04-05T11:21:53.724023Z","shell.execute_reply.started":"2022-04-05T11:21:53.698278Z","shell.execute_reply":"2022-04-05T11:21:53.72228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset folder\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import datasets,models,layers\nfrom tensorflow.keras.utils import to_categorical\n\nDATASET_PATH = '/kaggle/input/mais-202-winter-2022/'\n\n\nlb_to_str_csv = DATASET_PATH + 'label_int_to_str_mapping.csv'\ntrain_lb_csv = DATASET_PATH + 'train_labels.csv'\n\nlb_to_str_df = pd.read_csv(lb_to_str_csv)\nlb_to_str_df.head()\ntrain_lb_df = pd.read_csv(train_lb_csv)\ntrain_lb_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:49:35.472234Z","iopub.execute_input":"2022-04-05T11:49:35.473197Z","iopub.status.idle":"2022-04-05T11:49:35.503902Z","shell.execute_reply.started":"2022-04-05T11:49:35.473143Z","shell.execute_reply":"2022-04-05T11:49:35.503068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = np.load(DATASET_PATH+\"train_images.npy\")\ndef show_image(arr):\n    two_d = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n    plt.imshow(two_d, interpolation='nearest')\n    plt.show()\n\nshow_image(train_images[0]) # 0 is the index of the training image you want to display\n####################\nTRAIN_NPY = DATASET_PATH + 'train_images.npy'\nTEST_NPY = DATASET_PATH + 'test_images.npy'\ntrain_images = np.load(TRAIN_NPY)\ntest_images = np.load(TEST_NPY)\n# print(train_images[0])\nshow_img = np.reshape(train_images[5], (28,28))\ntrain_images = np.reshape(train_images, (-1, 28,28,1)) / 255\ntest_images = np.reshape(test_images, (-1, 28, 28,1)) / 255\nplt.imshow(show_img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:49:44.909939Z","iopub.execute_input":"2022-04-05T11:49:44.910233Z","iopub.status.idle":"2022-04-05T11:49:45.617855Z","shell.execute_reply.started":"2022-04-05T11:49:44.910203Z","shell.execute_reply":"2022-04-05T11:49:45.617044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ntrain_size = 40000\n\nlabels = train_lb_df['label'].tolist()\nx_train = train_images[:train_size]\ny_train = np.array(labels)[:train_size]\n\nx_val = train_images[train_size:]\ny_val = np.array(labels)[train_size:]\n\n\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)\nprint(x_train.shape, y_train.shape)\nprint(x_val.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:49:49.499164Z","iopub.execute_input":"2022-04-05T11:49:49.500065Z","iopub.status.idle":"2022-04-05T11:49:49.51734Z","shell.execute_reply.started":"2022-04-05T11:49:49.500016Z","shell.execute_reply":"2022-04-05T11:49:49.516468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_rows, img_cols = 28, 28\ninput_shape = (img_rows, img_cols, 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:49:58.073647Z","iopub.execute_input":"2022-04-05T11:49:58.0739Z","iopub.status.idle":"2022-04-05T11:49:58.078174Z","shell.execute_reply.started":"2022-04-05T11:49:58.073873Z","shell.execute_reply":"2022-04-05T11:49:58.077175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import BatchNormalization\nbatch_size = 256\nnum_classes = 10\nepochs = 20\n\n#input image dimensions\nimg_rows, img_cols = 28, 28\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 kernel_initializer='he_normal',\n                 input_shape=input_shape))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:51:11.821848Z","iopub.execute_input":"2022-04-05T11:51:11.823272Z","iopub.status.idle":"2022-04-05T11:51:11.907771Z","shell.execute_reply.started":"2022-04-05T11:51:11.823203Z","shell.execute_reply":"2022-04-05T11:51:11.90679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:51:15.381034Z","iopub.execute_input":"2022-04-05T11:51:15.38158Z","iopub.status.idle":"2022-04-05T11:51:15.392732Z","shell.execute_reply.started":"2022-04-05T11:51:15.381546Z","shell.execute_reply":"2022-04-05T11:51:15.392021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_val, y_val))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T11:51:38.047473Z","iopub.execute_input":"2022-04-05T11:51:38.048031Z","iopub.status.idle":"2022-04-05T11:59:01.14891Z","shell.execute_reply.started":"2022-04-05T11:51:38.047976Z","shell.execute_reply":"2022-04-05T11:59:01.147683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')","metadata":{"execution":{"iopub.status.busy":"2022-04-05T12:01:02.225793Z","iopub.execute_input":"2022-04-05T12:01:02.226119Z","iopub.status.idle":"2022-04-05T12:01:02.453137Z","shell.execute_reply.started":"2022-04-05T12:01:02.226087Z","shell.execute_reply":"2022-04-05T12:01:02.452477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = model.predict(test_images)\ny_test = np.argmax(y_test, axis=1)\n\ndf_test = pd.read_csv(DATASET_PATH+'sample_submission.csv')\ndf_test['label'] = y_test\ndf_test.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-05T12:16:08.89971Z","iopub.execute_input":"2022-04-05T12:16:08.900146Z","iopub.status.idle":"2022-04-05T12:16:12.718344Z","shell.execute_reply.started":"2022-04-05T12:16:08.900089Z","shell.execute_reply":"2022-04-05T12:16:12.717667Z"},"trusted":true},"execution_count":null,"outputs":[]}]}